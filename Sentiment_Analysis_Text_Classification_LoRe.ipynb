{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis/Text Classification- LoRe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSZ-0Uwp3Ho3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d29ccf2-60a3-40dd-cab7-d6ffab1d1246"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fiytjN83QG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ef4b5e27-7f98-4102-c28b-7705fcd61b94"
      },
      "source": [
        "#Download Data\n",
        "!gdown --id 1s-8A8sF7b23Tb9Myoc_3DTl6YXLpL17L  # train data\n",
        "!gdown --id 1EacvwnOHfwa4FiZy2K8mFpFjmpb4Mt-t #test data(no answer key)\n",
        "!gdown --id 1YtAHCzeZUXGZQ9cimdkkUq4lUk3ZH-I_  # evaluate.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1s-8A8sF7b23Tb9Myoc_3DTl6YXLpL17L\n",
            "To: /content/lab4_train.csv\n",
            "100% 331k/331k [00:00<00:00, 47.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EacvwnOHfwa4FiZy2K8mFpFjmpb4Mt-t\n",
            "To: /content/lab4_test.csv\n",
            "100% 36.2k/36.2k [00:00<00:00, 55.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YtAHCzeZUXGZQ9cimdkkUq4lUk3ZH-I_\n",
            "To: /content/evaluate.py\n",
            "100% 7.03k/7.03k [00:00<00:00, 13.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEm4NOav3V0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split Train&Dev\n",
        "data = pd.read_csv('lab4_train.csv') \n",
        "train, dev = np.split(data, [int(len(data)*0.8)]) # train:dev = 80:20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBnFRLi3yEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('lab4_test.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pSiD8N_4Fq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40ff8faf-0caa-4281-f895-2893180e266f"
      },
      "source": [
        "#Tokenization\n",
        "title_list = dev['text'].to_list()\n",
        "tokenized=[]\n",
        "for sentences in tqdm.tqdm(title_list):\n",
        "  tokenized.append(nltk.word_tokenize(sentences))\n",
        "dev['tokenized']=tokenized\n",
        "train_title_list = train['text'].to_list()\n",
        "train_tokenized=[]\n",
        "for sentences in tqdm.tqdm(train_title_list):\n",
        "  train_tokenized.append(nltk.word_tokenize(sentences))\n",
        "train['tokenized']=train_tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 632/632 [00:00<00:00, 6851.19it/s]\n",
            "100%|██████████| 2524/2524 [00:00<00:00, 7840.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_RLV9mBAIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemma = WordNetLemmatizer()\n",
        "def stem(li):\n",
        "  sli=[]\n",
        "  ssli=[]\n",
        "  for tk in li:\n",
        "    sli.append(lemma.lemmatize(tk, pos=\"n\"))\n",
        "  return(sli)\n",
        "dlt=dev['tokenized'].apply(stem)\n",
        "tlt=train['tokenized'].apply(stem)\n",
        "dev['tokenized']=dlt\n",
        "train['tokenized']=tlt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqQh97pWBZ_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bigram Creation\n",
        "def create_bigram(tokenized):\n",
        "  bigram = []\n",
        "  \n",
        "  if len(tokenized)>1:\n",
        "    for i in range(len(tokenized)-1):\n",
        "      bigram.append(tokenized[i]+' '+tokenized[i+1])\n",
        "  return bigram\n",
        "dev['bigram'] = dev['tokenized'].apply(create_bigram)\n",
        "train['bigram'] = train['tokenized'].apply(create_bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO-4lQ4y7GTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#New Dataframe for Prediction\n",
        "pred_df = pd.DataFrame()\n",
        "pred_df['id'] = dev['id']\n",
        "pred_df['tokenized']=dev['tokenized']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUSiHx7j4fTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Features\n",
        "def featurize(token_list):\n",
        "    features = {}\n",
        "    for token in token_list:\n",
        "        features[token] = 1\n",
        "    features['length'] = len(token_list)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmITkkSj4ibt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "30b3a897-f1c3-45e4-d180-fb12b8c4911a"
      },
      "source": [
        "#Train Model(Aspect Category)\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "vectorizer = DictVectorizer(sparse=True)\n",
        "train_features = train['tokenized'].apply(featurize)\n",
        "feature_vectors = vectorizer.fit_transform(train_features)\n",
        "lr_text_classifier = LogisticRegression()\n",
        "lr_text_classifier.fit(feature_vectors, train['aspectCategory'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiDaWjx25ZnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "dev_featurized_list_dicts = pred_df['tokenized'].apply(featurize)\n",
        "dev_feature_vector = vectorizer.transform(dev_featurized_list_dicts)\n",
        "predictions = lr_text_classifier.predict(dev_feature_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCcT65WL5tC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f2844bdd-c9d1-4b7e-d1dd-60a53a2caa32"
      },
      "source": [
        "#Report\n",
        "print (classification_report(dev['aspectCategory'], predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         precision    recall  f1-score   support\n",
            "\n",
            "               ambience       0.54      0.45      0.49        71\n",
            "anecdotes/miscellaneous       0.75      0.75      0.75       194\n",
            "                   food       0.63      0.77      0.69       203\n",
            "                  price       0.46      0.30      0.36        60\n",
            "                service       0.65      0.57      0.61       104\n",
            "\n",
            "               accuracy                           0.65       632\n",
            "              macro avg       0.61      0.57      0.58       632\n",
            "           weighted avg       0.64      0.65      0.64       632\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNHpwIgX7dE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df['aspectCategory']=predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRe6-mz71aw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b3755a07-74e9-4c04-cfe9-60fc513dc3de"
      },
      "source": [
        "#Train Model(Polarity)\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "vectorizer = DictVectorizer(sparse=True)\n",
        "train_features = train['tokenized'].apply(featurize)\n",
        "feature_vectors = vectorizer.fit_transform(train_features)\n",
        "lr_text_classifier = LogisticRegression()\n",
        "lr_text_classifier.fit(feature_vectors, train['polarity'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9dmNhyP7-p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "dev_featurized_list_dicts = pred_df['tokenized'].apply(featurize)\n",
        "dev_feature_vector = vectorizer.transform(dev_featurized_list_dicts)\n",
        "predictions = lr_text_classifier.predict(dev_feature_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNF_j5Pa8BKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1bc071d7-2718-424f-f801-0cf77a6beadb"
      },
      "source": [
        "#Report\n",
        "print (classification_report(dev['polarity'], predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict       0.30      0.28      0.29        25\n",
            "    negative       0.54      0.40      0.46       150\n",
            "     neutral       0.51      0.26      0.34        77\n",
            "    positive       0.74      0.89      0.81       380\n",
            "\n",
            "    accuracy                           0.67       632\n",
            "   macro avg       0.52      0.46      0.48       632\n",
            "weighted avg       0.65      0.67      0.65       632\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q15-1IM8Qms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df['polarity']=predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcUM5BIG8Ztj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "007a9275-f2b1-4fa9-a787-dc432a656674"
      },
      "source": [
        "# export to csv & evaluate\n",
        "pred_df.to_csv('pred.csv', index=None)\n",
        "!python3 evaluate.py lab4_train.csv pred.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== CLASSIFICATION : ASPECT ===\n",
            "                class name  precision  recall  F1-score support\n",
            "0                     food      0.839   0.768     0.802     203\n",
            "1                    price      0.783   0.300     0.434      60\n",
            "2                  service      0.855   0.567     0.682     104\n",
            "3                 ambience      0.780   0.451     0.571      71\n",
            "4  anecdotes/miscellaneous      0.820   0.753     0.785     194\n",
            "5                MACRO AVG      0.815   0.568     0.655     632\n",
            "6                MICRO AVG      0.827   0.650     0.728     632 \n",
            "\n",
            "=== CLASSIFICATION : SENTIMENT ===\n",
            "  class name  precision  recall  F1-score support\n",
            "0   positive      0.752   0.882     0.812     306\n",
            "1   negative      0.522   0.376     0.437     125\n",
            "2    neutral      0.606   0.270     0.374      74\n",
            "3   conflict      0.467   0.292     0.359      24\n",
            "4  MACRO AVG      0.587   0.455     0.496     529\n",
            "5  MICRO AVG      0.692   0.650     0.671     529 \n",
            "\n",
            "=== CLASSIFICATION : OVERALL ===\n",
            "              precision  recall  F1-score support\n",
            "0  MICRO AVG      0.584   0.459     0.514     632 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}